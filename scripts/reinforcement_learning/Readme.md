# CARLA Reinforcement Learning Parking Project

## Project Overview
This project demonstrates the use of basic reinforcement learning (RL) to control a vehicle in the CARLA simulator. The goal is for the vehicle to park at a specified location while avoiding collisions. The project implements a Q-learning algorithm, trains the agent, and provides an inference mode to control the vehicle using the trained model.

## Features
1. Custom RL Environment:

    * The environment uses the CARLA simulator.
    * State: The x and y coordinates of the vehicle.
    * Actions: Throttle and steering inputs.
    * Rewards:
        * Positive for reaching the goal.
        * Negative for collisions.
        * Penalized based on distance from the goal.

2. Q-Learning:
    * Discretizes the continuous state and action spaces for compatibility with Q-learning.
    * Saves and loads the Q-table for efficient training and inference.

3. Inference Mode:
    * Allows the agent to control the vehicle using a pre-trained Q-table without retraining.

## Prerequisites
* CARLA Simulator was used to simulate enviroment. used carla version is 09.15
* Python 3.8 or above

```bash
pip install carla numpy pickle
```

## Project Structure
    .
    â”œâ”€â”€ main.py           # Main script to train 
    â”œâ”€â”€ interence_mode.py # Inference mode to run already trained model 
    â”œâ”€â”€ CarlaParkingEnv   # Custom CARLA environment class
    â”œâ”€â”€ QLearningAgent    # Q-learning agent implementation
    â”œâ”€â”€ q_table.pkl       # Saved Q-table (generated after training)
    â””â”€â”€ README.md         # Project documentation

### Saving and Loading Q-Table
* Saving: The Q-table is saved automatically after each episode during training.
* Loading: During inference mode, the saved Q-table is loaded and used for action selection.

# Results
There are some giffs to demonstrate first iteration and inference mode in the carla.

This is first episodes on the simulation.

![Watch Video](videos/first_iteration.gif)

In the inference mode, vehicle succesfully reaches the target.

![Watch Video](videos/result.gif)



### TODOs
* Implement Deep Q-Learning for continuous state-action spaces.
* Add obstacle avoidance capabilities.
* Use policy-gradient methods like PPO for better generalization.

## Details of The Reinforcement Learning Theory In This Project.

**Q-Learning Update Rule**

The Q-value for a given state s  and action a is updated as follows:

$$
Q(s,a)â†Q(s,a)+Î±[r+Î³aâ€²maxQ(sâ€²,aâ€²)âˆ’Q(s,a)]
$$

Where:
ğ‘„(ğ‘ ,ğ‘) Current Q-value for state ğ‘  and action ğ›¼: Learning rate (0â‰¤ğ›¼â‰¤1).

ğ‘Ÿ: Immediate reward received after taking action ğ‘ in state ğ‘ .

ğ›¾: Discount factor (0â‰¤ğ›¾â‰¤1), which determines the importance of future rewards.

maxğ‘â€²ğ‘„(ğ‘ â€²,ğ‘â€²)â€‹: Maximum Q-value for the next state ğ‘ â€² over all possible actions ğ‘â€².


**Epsilon-Greedy Policy**

The action selection policy used in this project is the epsilon-greedy policy, defined as:

$$
r =
\begin{cases} 
+100 & \text{if the goal is reached (distance to goal $<$ threshold)}, \\ 
-50 & \text{if a collision occurs}, \\
-0.1 \times \text{distance to goal} & \text{otherwise}.
\end{cases}
$$


**State Discretization**
The continuous state space ğ‘  âˆˆ ğ‘…<sup>ğ‘›</sup>â€‹ is discretized into bins. For a given continuous variable ğ‘¥, its discrete bin index is computed as:
$$
bin(x)=digitize(x,bins)
$$

Where:

bins: Predefined thresholds dividing the continuous space into discrete intervals.

digitize(ğ‘¥,bins): Returns the index of the bin where ğ‘¥ belongs.

For the 2D state space [ğ‘¥,ğ‘¦], the discrete state is represented as:

$$
sdiscrete=(bin(x),bin(y))
$$

**Reward Function**
* +100: Vehicle reaches the goal within a small radius.
* -50: Vehicle experiences a collision.
* -0.1 * Distance to Goal: Penalized based on how far the vehicle is from the target.

**Training Objective**

The goal of the reinforcement learning agent is to learn a policy ğœ‹ that maximizes the cumulative discounted reward:
$$
Gtâ€‹ = k=âˆ‘Î³^kr_{t+k+1}
$$

Where:

ğºğ‘¡ : Total discounted reward starting from time step ğ‘¡.

ğ›¾: Discount factor determining the relative importance of future rewards.